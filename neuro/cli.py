"""
Neuro AGI Command Line Interface.

A neuroscience-inspired cognitive architecture with streaming responses.
Production-ready CLI with real-time token output.
"""

import argparse
import sys
import os
import time
import threading
import asyncio
from pathlib import Path
from typing import Optional, Dict, Any

# ANSI color codes
class Colors:
    RESET = "\033[0m"
    BOLD = "\033[1m"
    DIM = "\033[2m"

    # Colors
    CYAN = "\033[36m"
    GREEN = "\033[32m"
    YELLOW = "\033[33m"
    RED = "\033[31m"
    MAGENTA = "\033[35m"
    BLUE = "\033[34m"
    WHITE = "\033[37m"

    # Bright colors
    BRIGHT_CYAN = "\033[96m"
    BRIGHT_GREEN = "\033[92m"
    BRIGHT_YELLOW = "\033[93m"
    BRIGHT_MAGENTA = "\033[95m"
    BRIGHT_BLUE = "\033[94m"

    # Background
    BG_BLUE = "\033[44m"


class UI:
    """Terminal UI helpers."""

    # Box drawing
    TOP_LEFT = "â•­"
    TOP_RIGHT = "â•®"
    BOTTOM_LEFT = "â•°"
    BOTTOM_RIGHT = "â•¯"
    HORIZONTAL = "â”€"
    VERTICAL = "â”‚"

    WIDTH = 70

    @staticmethod
    def clear_line():
        sys.stdout.write("\r\033[K")
        sys.stdout.flush()

    @staticmethod
    def box_top(title=""):
        line = UI.TOP_LEFT + UI.HORIZONTAL * 2
        if title:
            line += f" {Colors.BOLD}{Colors.CYAN}{title}{Colors.RESET} "
            padding = UI.WIDTH - len(title) - 6
        else:
            padding = UI.WIDTH - 2
        line += UI.HORIZONTAL * padding + UI.TOP_RIGHT
        return line

    @staticmethod
    def box_bottom():
        return UI.BOTTOM_LEFT + UI.HORIZONTAL * (UI.WIDTH - 2) + UI.BOTTOM_RIGHT

    @staticmethod
    def box_line(text="", align="left"):
        # Strip ANSI codes for length calculation
        import re
        visible_text = re.sub(r'\033\[[0-9;]*m', '', text)
        padding = UI.WIDTH - len(visible_text) - 4

        if align == "center":
            left_pad = padding // 2
            right_pad = padding - left_pad
            return f"{UI.VERTICAL} {' ' * left_pad}{text}{' ' * right_pad} {UI.VERTICAL}"
        else:
            return f"{UI.VERTICAL} {text}{' ' * max(0, padding)} {UI.VERTICAL}"

    @staticmethod
    def divider():
        return f"{UI.VERTICAL}{UI.HORIZONTAL * (UI.WIDTH - 2)}{UI.VERTICAL}"


class Spinner:
    """Animated spinner for loading states."""

    FRAMES = ["â ‹", "â ™", "â ¹", "â ¸", "â ¼", "â ´", "â ¦", "â §", "â ‡", "â "]

    def __init__(self, message="Thinking"):
        self.message = message
        self.running = False
        self.thread = None
        self.frame = 0

    def start(self):
        self.running = True
        self.thread = threading.Thread(target=self._animate)
        self.thread.start()

    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join()
        UI.clear_line()

    def update(self, message: str):
        """Update spinner message."""
        self.message = message

    def _animate(self):
        while self.running:
            frame = self.FRAMES[self.frame % len(self.FRAMES)]
            sys.stdout.write(f"\r  {Colors.CYAN}{frame}{Colors.RESET} {self.message}...")
            sys.stdout.flush()
            self.frame += 1
            time.sleep(0.08)


def get_version():
    return "2.0.0"


def print_header():
    """Print the main header."""
    print()
    print(f"  {Colors.CYAN}{Colors.BOLD}â•­{'â”€' * 50}â•®{Colors.RESET}")
    print(f"  {Colors.CYAN}{Colors.BOLD}â”‚{Colors.RESET}{'NEURO AGI v2.0':^50}{Colors.CYAN}{Colors.BOLD}â”‚{Colors.RESET}")
    print(f"  {Colors.CYAN}{Colors.BOLD}â”‚{Colors.RESET}{Colors.DIM}{'Local AI That Learns From Your Code':^50}{Colors.RESET}{Colors.CYAN}{Colors.BOLD}â”‚{Colors.RESET}")
    print(f"  {Colors.CYAN}{Colors.BOLD}â•°{'â”€' * 50}â•¯{Colors.RESET}")
    print()


def show_cognitive_phase(phase: str, details: str = ""):
    """Display cognitive phase indicator."""
    icons = {
        "perceive": "ğŸ‘",
        "think": "ğŸ’­",
        "act": "âš¡",
        "learn": "ğŸ“š",
        "improve": "ğŸ”§",
    }
    icon = icons.get(phase.lower(), "â—")
    if details:
        print(f"  {Colors.DIM}{icon} {phase.upper()}: {details}{Colors.RESET}")
    else:
        print(f"  {Colors.DIM}{icon} {phase.upper()}{Colors.RESET}")


class StatusLine:
    """Interactive status line that updates in place (like Claude Code)."""

    def __init__(self):
        self.running = False
        self.thread = None
        self.engine = None
        self.last_status = ""
        self.spinner_frame = 0
        self.spinner_chars = ["â ‹", "â ™", "â ¹", "â ¸", "â ¼", "â ´", "â ¦", "â §", "â ‡", "â "]

    def start(self, engine):
        """Start the status line updater."""
        self.engine = engine
        self.running = True
        self.thread = threading.Thread(target=self._update_loop, daemon=True)
        self.thread.start()

    def stop(self):
        """Stop the status line updater."""
        self.running = False
        if self.thread:
            self.thread.join(timeout=1)
        # Clear the status line
        sys.stdout.write("\r" + " " * 80 + "\r")
        sys.stdout.flush()

    def _get_status(self) -> str:
        """Get current status from autonomous loop."""
        if not self.engine or not self.engine.autonomous:
            return ""

        try:
            auto = self.engine.autonomous
            if not auto.running:
                return ""

            # Get current activity
            if auto.is_busy:
                if not auto.initial_benchmark_done:
                    return f"{Colors.YELLOW}Running initial benchmark...{Colors.RESET}"

                focus = auto._current_focus
                if focus:
                    learned = auto._focus_learned
                    return f"{Colors.CYAN}Learning from {focus}{Colors.RESET} ({learned} facts)"

            # Show cycle progress
            if self.engine.evolution:
                evo = self.engine.evolution.get_stats()
                cycle = evo.get('cycle', 0)
                facts = evo.get('facts_this_cycle', 0)
                total = evo.get('total_facts', 0)

                # Progress bar
                progress = facts / 100
                bar_width = 15
                filled = int(bar_width * progress)
                bar = "â–ˆ" * filled + "â–‘" * (bar_width - filled)

                if facts > 0:
                    return f"{Colors.GREEN}Cycle {cycle}{Colors.RESET} [{bar}] {facts}/100 facts | total: {total}"

            return ""
        except Exception:
            return ""

    def _update_loop(self):
        """Background loop to update status line."""
        while self.running:
            try:
                status = self._get_status()
                if status and status != self.last_status:
                    spinner = self.spinner_chars[self.spinner_frame % len(self.spinner_chars)]
                    # Save cursor, move to status line, print, restore cursor
                    line = f"\r  {Colors.DIM}{spinner}{Colors.RESET} {status}"
                    # Pad to clear old content
                    line += " " * max(0, 80 - len(line))
                    sys.stdout.write(f"\033[s\033[999;1H{line}\033[u")
                    sys.stdout.flush()
                    self.last_status = status
                    self.spinner_frame += 1
            except Exception:
                pass
            time.sleep(0.5)


class ActivityDisplay:
    """Real-time activity display for autonomous learning (like Claude Code)."""

    ICONS = {
        "benchmark": "ğŸ“Š",
        "benchmark_done": "âœ…",
        "fetching": "ğŸ”",
        "focus": "ğŸ¯",
        "analyzing": "ğŸ§ ",
        "learned": "ğŸ“š",
        "skip": "â­ï¸",
        "reflection": "ğŸ’­",
        "training": "ğŸ”§",
        "training_done": "âœ¨",
        "training_check": "ğŸ”„",
        "training_skip": "â¸ï¸",
        "new_cycle": "ğŸ”„",
        "error": "âŒ",
        "ocr": "ğŸ‘",
        "image": "ğŸ–¼ï¸",
    }

    def __init__(self):
        self.last_activity = ""
        self.activity_count = 0

    def on_activity(self, activity_type: str, message: str, data: dict):
        """Callback for activity notifications - displays in terminal."""
        icon = self.ICONS.get(activity_type, "â—")

        # Color based on type
        if activity_type in ["learned", "benchmark_done", "training_done"]:
            color = Colors.GREEN
        elif activity_type in ["focus", "new_cycle"]:
            color = Colors.BRIGHT_CYAN
        elif activity_type in ["analyzing", "fetching"]:
            color = Colors.CYAN
        elif activity_type in ["skip"]:
            color = Colors.DIM
        elif activity_type in ["error"]:
            color = Colors.RED
        else:
            color = Colors.YELLOW

        # Format message based on type
        if activity_type == "learned":
            # Show detailed learning info
            topic = data.get('topic', 'Unknown')
            source = data.get('source', 'Unknown')
            progress = data.get('cycle_progress', 0)
            total = data.get('total_facts', 0)

            # Progress bar
            bar_width = 20
            filled = int(bar_width * (progress / 100))
            bar = f"{Colors.GREEN}{'â–ˆ' * filled}{Colors.DIM}{'â–‘' * (bar_width - filled)}{Colors.RESET}"

            print(f"\n  {icon} {color}LEARNED{Colors.RESET} [{bar}] {progress}/100")
            print(f"     {Colors.CYAN}Topic:{Colors.RESET} {topic[:50]}")
            print(f"     {Colors.DIM}Source: {source} | Total: {total} facts{Colors.RESET}")

        elif activity_type == "benchmark_done":
            score = data.get('score', 0)
            weak = data.get('weak_areas', [])
            print(f"\n  {icon} {color}BENCHMARK COMPLETE{Colors.RESET}")
            print(f"     {Colors.CYAN}Score:{Colors.RESET} {score:.1%}")
            if weak:
                weak_str = ", ".join([f"{w[0]}: {w[1]:.0%}" for w in weak[:3]])
                print(f"     {Colors.YELLOW}Weak areas:{Colors.RESET} {weak_str}")

        elif activity_type == "focus":
            source = data.get('source', 'Unknown')
            items = data.get('items', 0)
            print(f"\n  {icon} {color}FOCUS:{Colors.RESET} {source}")
            print(f"     {Colors.DIM}Processing {items} items...{Colors.RESET}")

        elif activity_type == "new_cycle":
            cycle = data.get('cycle', 0)
            print(f"\n  {icon} {color}NEW LEARNING CYCLE {cycle}{Colors.RESET}")
            print(f"     {Colors.DIM}Target: 100 unique facts{Colors.RESET}")

        elif activity_type == "reflection":
            # Show reflection summary
            print(f"\n  {icon} {color}REFLECTION{Colors.RESET}")
            # Parse reflection message for key stats
            lines = message.strip().split('\n')
            for line in lines[:5]:
                if line.strip() and not line.startswith('='):
                    print(f"     {Colors.DIM}{line.strip()}{Colors.RESET}")

        elif activity_type in ["skip", "fetching", "analyzing"]:
            # Compact display for frequent activities - inline update
            short_msg = message[:55] + "..." if len(message) > 55 else message
            sys.stdout.write(f"\r  {icon} {color}{short_msg}{Colors.RESET}" + " " * 20)
            sys.stdout.flush()
            self.last_activity = message
            self.activity_count += 1
            return  # Don't print newline

        else:
            # Generic display
            print(f"\n  {icon} {color}{message}{Colors.RESET}")

        self.last_activity = message
        self.activity_count += 1
        # Reprint prompt hint
        sys.stdout.write(f"\n  {Colors.DIM}(autonomous learning active){Colors.RESET}")
        sys.stdout.flush()


class LearningNotifier:
    """Shows real-time learning notifications (like Claude Code activity)."""

    def __init__(self):
        self.last_facts = 0
        self.last_cycle = 0
        self.notifications = []

    def check_and_notify(self, engine) -> List[str]:
        """Check for new learning events and return notifications."""
        notifications = []

        if not engine or not engine.evolution:
            return notifications

        try:
            evo = engine.evolution.get_stats()
            facts = evo.get('total_facts', 0)
            cycle = evo.get('cycle', 0)
            facts_this_cycle = evo.get('facts_this_cycle', 0)

            # New facts learned
            if facts > self.last_facts:
                diff = facts - self.last_facts
                notifications.append(f"{Colors.GREEN}+{diff} facts learned{Colors.RESET} (total: {facts})")
                self.last_facts = facts

            # New cycle started
            if cycle > self.last_cycle:
                notifications.append(f"{Colors.BRIGHT_CYAN}Started learning cycle {cycle}{Colors.RESET}")
                self.last_cycle = cycle

            # Milestone notifications
            if facts_this_cycle == 50 and self.last_facts < facts:
                notifications.append(f"{Colors.YELLOW}Halfway to benchmark! (50/100 facts){Colors.RESET}")

            if facts_this_cycle >= 100:
                notifications.append(f"{Colors.BRIGHT_GREEN}Cycle complete! Running benchmark...{Colors.RESET}")

            # Check autonomous loop state
            if engine.autonomous:
                auto = engine.autonomous
                if auto.initial_benchmark_done and auto.benchmark_results:
                    score = auto.benchmark_results.get('avg_score', 0)
                    weak = auto.weak_areas
                    if weak and len(notifications) == 0:
                        # Occasionally remind about weak areas
                        area, _ = weak[0]
                        notifications.append(f"{Colors.DIM}Focusing on: {area}{Colors.RESET}")

        except Exception:
            pass

        return notifications


def cmd_chat(args):
    """Chat with Neuro AGI using streaming responses."""
    print_header()

    # Check Ollama
    spinner = Spinner("Connecting to Ollama")
    spinner.start()

    ollama_available = False
    models = []

    try:
        import requests
        r = requests.get("http://localhost:11434/api/tags", timeout=5)
        if r.status_code == 200:
            ollama_available = True
            models = [m["name"] for m in r.json().get("models", [])]
    except Exception:
        pass

    spinner.stop()

    # Initialize the new NEURO Engine
    engine = None
    engine_available = False

    try:
        from engine import NeuroEngine, EngineConfig
        config = EngineConfig(
            model="ministral-3:8b",
            show_thinking=True,
            verbose=False
        )
        engine = NeuroEngine(config)
        engine_available = True
    except ImportError:
        # Fallback: try relative import
        try:
            sys.path.insert(0, str(Path(__file__).parent))
            from engine import NeuroEngine, EngineConfig
            config = EngineConfig(
                model="ministral-3:8b",
                show_thinking=True,
                verbose=False
            )
            engine = NeuroEngine(config)
            engine_available = True
        except ImportError:
            pass

    # Initialize tools (for fallback mode)
    tools = None
    tools_available = False
    try:
        sys.path.insert(0, str(Path(__file__).parent.parent / "neuro-model" / "src"))
        from tools import Tools
        tools = Tools(work_dir=os.getcwd())
        tools_available = True
    except ImportError:
        pass

    # Initialize Cognitive Pipeline
    pipeline = None
    pipeline_available = False
    try:
        from cognitive_pipeline import CognitivePipeline
        pipeline = CognitivePipeline(verbose=False)
        pipeline_available = True
    except ImportError:
        pass

    # Initialize autonomous learning via engine
    learner = None
    learner_available = False
    autonomous_started = False
    activity_display = ActivityDisplay()

    if engine_available:
        # Set up activity callback for real-time display
        engine.set_activity_callback(activity_display.on_activity)

        # Start autonomous loop from engine
        if engine.start_autonomous():
            autonomous_started = True
            learner_available = True
        # Also check if we have trainer/evolution directly
        if engine.trainer:
            learner_available = True

    # Initialize UltraThink
    ultrathink = None
    ultrathink_available = False
    if pipeline_available and pipeline.ultrathink:
        ultrathink = pipeline.ultrathink
        ultrathink_available = True
    else:
        try:
            from ultrathink import UltraThink
            ultrathink = UltraThink(verbose=False)
            ultrathink_available = True
        except ImportError:
            pass

    # Initialize Curiosity Loop (DISABLED - replaced by autonomous learning)
    curiosity_loop = None
    curiosity_available = False
    # Old curiosity loop is replaced by the new autonomous learning system
    # which is integrated into the engine

    # Initialize Self-Improving Agent
    self_improver = None
    self_improver_available = False
    try:
        from self_improving_agent import SelfImprovingAgent
        self_improver = SelfImprovingAgent()
        self_improver_available = True
    except ImportError:
        pass

    # Status display
    if ollama_available and models:
        model = "ministral-3:8b"
        print(f"  {Colors.GREEN}â—{Colors.RESET} Ollama: {Colors.GREEN}connected{Colors.RESET}")
        print(f"  {Colors.DIM}  Model: {model}{Colors.RESET}")
    else:
        print(f"  {Colors.YELLOW}â—{Colors.RESET} Ollama: {Colors.YELLOW}not available{Colors.RESET}")
        print(f"  {Colors.DIM}  Run: ollama serve && ollama pull ministral-3:8b{Colors.RESET}")

    if engine_available:
        stats = engine.get_stats()
        print(f"  {Colors.BRIGHT_GREEN}â—{Colors.RESET} NEURO Engine: {Colors.BRIGHT_GREEN}STREAMING ENABLED{Colors.RESET}")
        if stats.get('pipeline_available'):
            print(f"  {Colors.DIM}  â””â”€ Cognitive Pipeline: active{Colors.RESET}")
        if stats.get('emotions_available'):
            emo = stats.get('emotion', {})
            print(f"  {Colors.DIM}  â””â”€ Emotions: {emo.get('dominant', 'neutral')}{Colors.RESET}")
        if stats.get('social_available'):
            print(f"  {Colors.DIM}  â””â”€ Social Cognition: active{Colors.RESET}")
        if stats.get('embodied_available'):
            print(f"  {Colors.DIM}  â””â”€ Embodied Cognition: active{Colors.RESET}")
        if stats.get('git_repo'):
            print(f"  {Colors.DIM}  â””â”€ Git: repository detected{Colors.RESET}")
    elif pipeline_available:
        pstats = pipeline.get_stats()
        cognitive_count = pstats.get('cognitive_modules', 0)
        pipeline_count = pstats.get('pipeline_components', len(pstats.get('active_components', [])))
        total = pstats.get('num_components', cognitive_count + pipeline_count)
        print(f"  {Colors.GREEN}â—{Colors.RESET} Cognitive Pipeline: {Colors.GREEN}{total} components{Colors.RESET}")

    if tools_available:
        print(f"  {Colors.GREEN}â—{Colors.RESET} Tools: {Colors.GREEN}13 tools available{Colors.RESET}")

    if learner_available and engine_available:
        if engine.trainer:
            trainer_stats = engine.trainer.get_stats()
            fact_count = trainer_stats.get('total_facts', 0)
            topic_count = trainer_stats.get('topics_count', 0)
            if autonomous_started:
                print(f"  {Colors.BRIGHT_GREEN}â—{Colors.RESET} Autonomous: {Colors.BRIGHT_GREEN}READY{Colors.RESET} | {fact_count} facts | {topic_count} topics")
                print(f"  {Colors.DIM}    â””â”€ Will learn when idle (10s after your last message){Colors.RESET}")
            else:
                print(f"  {Colors.GREEN}â—{Colors.RESET} Learning: {Colors.GREEN}{fact_count} facts{Colors.RESET} | {topic_count} topics")

    if self_improver_available:
        si_stats = self_improver.get_stats()
        print(f"  {Colors.BRIGHT_BLUE}â—{Colors.RESET} Self-Improver: {Colors.BRIGHT_BLUE}{si_stats['learned_patterns']} patterns{Colors.RESET}, {si_stats['error_solutions']} solutions")

    print()
    print(f"  {Colors.DIM}Commands: /think /tools /learn /benchmark /status /edit /git /ocr /clear /quit{Colors.RESET}")
    print(f"  {Colors.DIM}{'â”€' * 50}{Colors.RESET}")

    history = []
    current_model = "ministral-3:8b"

    # Async event loop for streaming
    loop = asyncio.new_event_loop()

    def cleanup():
        """Cleanup on exit."""
        try:
            if engine_available:
                loop.run_until_complete(engine.close())
            if pipeline_available:
                pipeline.save()
                pstats = pipeline.get_stats()
                if 'knowledge' in pstats:
                    print(f"  {Colors.GREEN}Knowledge: {pstats['knowledge']['total_facts']} facts saved{Colors.RESET}")
        except Exception:
            pass
        # Autonomous loop is stopped by engine.close()
        try:
            if engine_available and engine.trainer:
                stats = engine.trainer.get_stats()
                print(f"  {Colors.DIM}Saved {stats.get('total_facts', 0)} facts{Colors.RESET}")
        except Exception:
            pass
        print(f"  {Colors.DIM}Goodbye!{Colors.RESET}\n")
        loop.close()

    while True:
        try:
            user_input = input(f"\n  {Colors.BRIGHT_CYAN}You:{Colors.RESET} ").strip()
        except (EOFError, KeyboardInterrupt):
            print()
            cleanup()
            break

        if not user_input:
            continue

        # Commands
        if user_input in ("/quit", "/exit"):
            cleanup()
            break

        if user_input == "/clear":
            history.clear()
            print(f"  {Colors.DIM}Conversation cleared{Colors.RESET}")
            continue

        if user_input == "/status":
            print(f"\n  {Colors.BOLD}{Colors.CYAN}NEURO AGI v2.0 - STATUS{Colors.RESET}")
            print(f"  {Colors.DIM}{'â•' * 55}{Colors.RESET}")

            if ollama_available:
                print(f"  {Colors.GREEN}â—{Colors.RESET} Ollama: connected ({current_model})")
            else:
                print(f"  {Colors.YELLOW}â—{Colors.RESET} Ollama: not available")

            if engine_available:
                stats = engine.get_stats()
                print(f"  {Colors.BRIGHT_GREEN}â—{Colors.RESET} Engine: streaming enabled")
                print(f"    â””â”€ Model: {stats['model']}")
                print(f"    â””â”€ Tools: {'available' if stats['tools_available'] else 'not loaded'}")
                print(f"    â””â”€ Pipeline: {'active' if stats['pipeline_available'] else 'not loaded'}")
                print(f"    â””â”€ Git repo: {'yes' if stats['git_repo'] else 'no'}")

            if pipeline_available:
                pstats = pipeline.get_stats()
                if 'knowledge' in pstats:
                    kb = pstats['knowledge']
                    print(f"  {Colors.GREEN}â—{Colors.RESET} Knowledge: {kb['total_facts']} facts")
                if 'ultrathink' in pstats:
                    ut = pstats['ultrathink']
                    print(f"  {Colors.GREEN}â—{Colors.RESET} UltraThink: {ut['modules_loaded']} modules")

            if learner_available and engine_available:
                if engine.trainer:
                    trainer_stats = engine.trainer.get_stats()
                    print(f"  {Colors.GREEN}â—{Colors.RESET} Learning: {trainer_stats.get('total_facts', 0)} facts")
                if engine.evolution:
                    evo_stats = engine.evolution.get_stats()
                    print(f"  {Colors.CYAN}â—{Colors.RESET} Evolution: cycle {evo_stats.get('cycle', 0)}, {evo_stats.get('total_facts', 0)} unique facts")
                    improvement = evo_stats.get('improvement', 0)
                    if improvement:
                        print(f"    â””â”€ Improvement: {improvement:+.1%}")
                if autonomous_started and engine.autonomous:
                    auto_stats = engine.autonomous.get_stats()
                    status = "RUNNING" if auto_stats.get('running', False) else "STOPPED"
                    print(f"  {Colors.BRIGHT_GREEN}â—{Colors.RESET} Autonomous: {status}")
                    if auto_stats.get('initial_benchmark_done'):
                        weak = auto_stats.get('weak_areas', [])
                        if weak:
                            weak_str = ", ".join([f"{w[0]}: {w[1]:.0%}" for w in weak[:3]])
                            print(f"    â””â”€ Weak areas: {weak_str}")

            # Curiosity loop replaced by autonomous learning

            print(f"  {Colors.DIM}â—{Colors.RESET} Session: {len(history)} messages")
            print()
            continue

        if user_input == "/tools":
            print(f"\n  {Colors.BOLD}Available Tools:{Colors.RESET}")
            print(f"  {Colors.DIM}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{Colors.RESET}")
            tool_list = [
                ("web_search", "Search the web"),
                ("web_fetch", "Fetch webpage content"),
                ("github_user", "Get GitHub user profile"),
                ("github_repos", "List user's repositories"),
                ("read_file", "Read file contents"),
                ("write_file", "Write to a file"),
                ("edit_file", "Edit specific lines (with diff)"),
                ("list_files", "List directory"),
                ("run_command", "Execute shell command"),
                ("run_python", "Run Python code"),
                ("remember", "Store in memory"),
                ("recall", "Retrieve from memory"),
                ("git_commit", "Safe git commit"),
                ("ocr_read", "Read text from image (DeepSeek OCR)"),
                ("ocr_analyze", "Analyze image and answer questions"),
                ("ocr_code", "Extract code from screenshot"),
            ]
            for name, desc in tool_list:
                print(f"  {Colors.CYAN}{name:15}{Colors.RESET} {desc}")
            print()
            continue

        if user_input == "/learn":
            if engine_available and engine.trainer:
                trainer_stats = engine.trainer.get_stats()
                print(f"\n  {Colors.BOLD}Learning State:{Colors.RESET}")
                print(f"  {Colors.DIM}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{Colors.RESET}")
                print(f"  {Colors.CYAN}Total Facts:{Colors.RESET} {trainer_stats.get('total_facts', 0)}")
                print(f"  {Colors.CYAN}Topics:{Colors.RESET} {trainer_stats.get('topics_count', 0)}")

                if engine.evolution:
                    evo = engine.evolution.get_stats()
                    print(f"  {Colors.CYAN}Unique Facts:{Colors.RESET} {evo.get('total_facts', 0)}")
                    print(f"  {Colors.CYAN}Learning Cycle:{Colors.RESET} {evo.get('cycle', 0)}")
                    print(f"  {Colors.CYAN}Facts This Cycle:{Colors.RESET} {evo.get('facts_this_cycle', 0)}/100")
                    if evo.get('baseline_score'):
                        print(f"  {Colors.CYAN}Baseline Score:{Colors.RESET} {evo['baseline_score']:.1%}")
                    if evo.get('current_score'):
                        print(f"  {Colors.CYAN}Current Score:{Colors.RESET} {evo['current_score']:.1%}")
                    if evo.get('improvement'):
                        imp = evo['improvement']
                        color = Colors.GREEN if imp > 0 else Colors.RED
                        print(f"  {Colors.CYAN}Improvement:{Colors.RESET} {color}{imp:+.1%}{Colors.RESET}")

                print()
            else:
                print(f"  {Colors.DIM}Learning system not available{Colors.RESET}")
            continue

        if user_input == "/benchmark":
            if engine_available and engine.benchmark:
                print(f"\n  {Colors.BOLD}{Colors.YELLOW}Running Benchmark...{Colors.RESET}")
                print(f"  {Colors.DIM}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{Colors.RESET}")

                def think_fn(q):
                    knowledge = ""
                    if engine.trainer:
                        knowledge = engine.trainer.get_knowledge_for_prompt(q)
                    if knowledge:
                        enhanced = f"{knowledge}\n\nQuestion: {q}\nThink step by step:"
                    else:
                        enhanced = f"Question: {q}\nThink step by step:"
                    return loop.run_until_complete(engine.chat(enhanced, stream_to_terminal=False))

                results = engine.benchmark.run_benchmark(think_fn, "cli-test")

                avg_score = results.get('avg_score', 0)
                print(f"\n  {Colors.BOLD}Results:{Colors.RESET}")
                print(f"  {Colors.CYAN}Average Score:{Colors.RESET} {avg_score:.1%}")

                # Show by category
                category_scores = {}
                for test in results.get('tests', []):
                    cat = test.get('category', 'unknown')
                    score = test.get('score', 0)
                    if cat not in category_scores:
                        category_scores[cat] = []
                    category_scores[cat].append(score)

                print(f"  {Colors.CYAN}By Category:{Colors.RESET}")
                for cat, scores in sorted(category_scores.items()):
                    avg = sum(scores) / len(scores) if scores else 0
                    color = Colors.GREEN if avg >= 0.7 else Colors.YELLOW if avg >= 0.4 else Colors.RED
                    print(f"    {cat:15} {color}{avg:.0%}{Colors.RESET}")

                # Record in evolution if available
                if engine.evolution:
                    engine.evolution.record_benchmark(avg_score, {
                        'categories': {k: sum(v)/len(v) for k, v in category_scores.items()},
                        'source': 'manual'
                    })
                    print(f"\n  {Colors.GREEN}Benchmark recorded in evolution tracker{Colors.RESET}")

                print()
            else:
                print(f"  {Colors.DIM}Benchmark not available{Colors.RESET}")
            continue

        if user_input == "/curiosity" or user_input == "/autonomous":
            if autonomous_started and engine_available and engine.autonomous:
                auto = engine.autonomous
                stats = auto.get_stats()

                # Determine actual status
                if not stats.get('running', False):
                    status_str = f"{Colors.RED}STOPPED{Colors.RESET}"
                elif auto.paused:
                    idle = time.time() - auto.last_user_activity
                    remaining = max(0, auto.idle_threshold - idle)
                    status_str = f"{Colors.YELLOW}PAUSED{Colors.RESET} (resumes in {remaining:.0f}s)"
                else:
                    status_str = f"{Colors.BRIGHT_GREEN}LEARNING{Colors.RESET}"

                print(f"\n  {Colors.BOLD}{Colors.BRIGHT_GREEN}Autonomous Learning Loop:{Colors.RESET}")
                print(f"  {Colors.DIM}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{Colors.RESET}")
                print(f"  {Colors.BRIGHT_GREEN}Status:{Colors.RESET} {status_str}")
                print(f"  {Colors.BRIGHT_GREEN}Initial Benchmark:{Colors.RESET} {'Done' if stats.get('initial_benchmark_done') else 'Pending'}")
                print(f"  {Colors.BRIGHT_GREEN}Current Focus:{Colors.RESET} {stats.get('current_focus') or 'None'}")
                print(f"  {Colors.BRIGHT_GREEN}Focus Learned:{Colors.RESET} {stats.get('focus_learned', 0)} facts")

                # Show last learned topic
                if auto.last_learned_topic:
                    print(f"  {Colors.CYAN}Last Learned:{Colors.RESET} {auto.last_learned_topic[:50]}")
                    print(f"  {Colors.DIM}    from {auto.last_learned_source}{Colors.RESET}")

                evo = stats.get('evolution', {})
                if evo:
                    print(f"  {Colors.CYAN}Learning Cycle:{Colors.RESET} {evo.get('cycle', 0)}")
                    print(f"  {Colors.CYAN}Unique Facts:{Colors.RESET} {evo.get('total_facts', 0)}")
                    progress = evo.get('facts_this_cycle', 0)
                    bar_w = 20
                    filled = int(bar_w * (progress / 100))
                    bar = f"{'â–ˆ' * filled}{'â–‘' * (bar_w - filled)}"
                    print(f"  {Colors.CYAN}Cycle Progress:{Colors.RESET} [{bar}] {progress}/100")

                weak = stats.get('weak_areas', [])
                if weak:
                    print(f"  {Colors.YELLOW}Weak Areas:{Colors.RESET}")
                    for area, score in weak[:5]:
                        print(f"    - {area}: {score:.0%}")

                # Show recent activity
                if auto.activity_log:
                    print(f"\n  {Colors.DIM}Recent Activity:{Colors.RESET}")
                    for a in auto.activity_log[-3:]:
                        icon = ActivityDisplay.ICONS.get(a['type'], 'â—')
                        print(f"    {icon} {a['message'][:50]}")

                print()
            else:
                print(f"  {Colors.DIM}Autonomous learning not available{Colors.RESET}")
            continue

        # /think <question> - Deep reasoning with UltraThink
        if user_input.startswith("/think"):
            if not ultrathink_available:
                print(f"  {Colors.DIM}UltraThink not available{Colors.RESET}")
                continue

            question = user_input[6:].strip()
            if not question:
                print(f"  {Colors.DIM}Usage: /think <question>{Colors.RESET}")
                continue

            print(f"\n  {Colors.MAGENTA}[UltraThink]{Colors.RESET} Deep reasoning...\n")
            result = ultrathink.think(question)

            print(f"  {Colors.BOLD}Problem Analysis:{Colors.RESET}")
            print(f"  {Colors.DIM}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{Colors.RESET}")

            for step in result.reasoning_chain:
                print(f"  {Colors.CYAN}{step.module}:{Colors.RESET} {step.thought}")
                for insight in step.insights:
                    print(f"    {Colors.DIM}â†’ {insight}{Colors.RESET}")

            print(f"\n  {Colors.BOLD}Confidence:{Colors.RESET} {result.confidence:.0%}")
            print(f"  {Colors.BOLD}Modules Used:{Colors.RESET} {', '.join(result.modules_used)}")
            print()
            continue

        # /edit <file> - Edit a file with diff preview
        if user_input.startswith("/edit "):
            if not engine_available:
                print(f"  {Colors.DIM}Engine not available for editing{Colors.RESET}")
                continue

            file_path = user_input[6:].strip()
            if not file_path:
                print(f"  {Colors.DIM}Usage: /edit <file_path>{Colors.RESET}")
                continue

            # Read the file
            if os.path.exists(file_path):
                with open(file_path, 'r') as f:
                    content = f.read()
                print(f"\n  {Colors.CYAN}File:{Colors.RESET} {file_path}")
                print(f"  {Colors.DIM}Lines: {len(content.splitlines())}{Colors.RESET}")
                print(f"  {Colors.DIM}Ask me to make specific edits (e.g., 'change line 5 to ...'):{Colors.RESET}")
            else:
                print(f"  {Colors.RED}File not found: {file_path}{Colors.RESET}")
            continue

        # /git - Git operations
        if user_input.startswith("/git"):
            if not engine_available:
                print(f"  {Colors.DIM}Engine not available for git{Colors.RESET}")
                continue

            git_cmd = user_input[4:].strip()
            if not git_cmd:
                # Show git status
                if engine.git.is_repo():
                    changes = engine.git.get_status()
                    branch = engine.git.get_current_branch()
                    print(f"\n  {Colors.BOLD}Git Status:{Colors.RESET}")
                    print(f"  {Colors.DIM}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{Colors.RESET}")
                    print(f"  {Colors.CYAN}Branch:{Colors.RESET} {branch}")
                    print(f"  {Colors.CYAN}Changes:{Colors.RESET} {len(changes)} files")
                    for c in changes[:10]:
                        staged = "[+]" if c.staged else "[ ]"
                        print(f"    {staged} {c.status} {c.path}")
                    print()
                else:
                    print(f"  {Colors.DIM}Not a git repository{Colors.RESET}")
                continue

            if git_cmd == "commit":
                # Smart commit
                if engine.git.is_repo():
                    print(f"\n  {Colors.CYAN}Generating commit message...{Colors.RESET}")
                    msg = engine.git.generate_commit_message()
                    print(f"  {Colors.GREEN}Suggested:{Colors.RESET} {msg}")
                    confirm = input(f"  {Colors.DIM}Commit with this message? [Y/n]:{Colors.RESET} ").strip().lower()
                    if confirm in ('', 'y', 'yes'):
                        changes = engine.git.get_status()
                        files = [c.path for c in changes if c.status != '?']
                        result = loop.run_until_complete(engine.commit_changes(files, msg))
                        if result.status.value == "success":
                            print(f"  {Colors.GREEN}Committed!{Colors.RESET}")
                        else:
                            print(f"  {Colors.RED}{result.message}{Colors.RESET}")
                continue

        # /ocr <image> [question] - Read text from image using DeepSeek OCR
        if user_input.startswith("/ocr"):
            if not engine_available or not engine.ocr:
                print(f"  {Colors.DIM}DeepSeek OCR not available{Colors.RESET}")
                print(f"  {Colors.DIM}Make sure deepseek-ocr:latest is installed in Ollama{Colors.RESET}")
                continue

            parts = user_input[4:].strip().split(maxsplit=1)
            if not parts:
                print(f"  {Colors.DIM}Usage: /ocr <image_path> [question]{Colors.RESET}")
                print(f"  {Colors.DIM}Examples:{Colors.RESET}")
                print(f"    {Colors.CYAN}/ocr screenshot.png{Colors.RESET}              - Extract all text")
                print(f"    {Colors.CYAN}/ocr diagram.png what does this show?{Colors.RESET} - Analyze image")
                print(f"    {Colors.CYAN}/ocr code.png --code{Colors.RESET}             - Extract code only")
                continue

            image_path = parts[0]
            question = parts[1] if len(parts) > 1 else None

            if not os.path.exists(image_path):
                print(f"  {Colors.RED}Image not found: {image_path}{Colors.RESET}")
                continue

            print(f"\n  {Colors.CYAN}Processing image with DeepSeek OCR...{Colors.RESET}")

            try:
                result = None
                if question == "--code":
                    # Extract code from screenshot
                    result = engine.ocr.extract_code(image_path)
                    print(f"\n  {Colors.BOLD}Extracted Code:{Colors.RESET}")
                    print(f"  {Colors.DIM}{'â”€' * 55}{Colors.RESET}")
                    for line in result.text.split('\n'):
                        print(f"  {Colors.GREEN}{line}{Colors.RESET}")
                elif question == "--learn":
                    # Learn from image
                    success = engine.learn_from_image(image_path)
                    if success:
                        print(f"  {Colors.GREEN}Learned from image!{Colors.RESET}")
                    else:
                        print(f"  {Colors.YELLOW}Could not extract learnable content{Colors.RESET}")
                elif question:
                    # Analyze with question
                    result = engine.ocr.analyze_image(image_path, question)
                    print(f"\n  {Colors.BOLD}Analysis:{Colors.RESET}")
                    print(f"  {Colors.DIM}{'â”€' * 55}{Colors.RESET}")
                    for line in result.text.split('\n'):
                        print(f"  {line}")
                else:
                    # Extract all text
                    result = engine.ocr.read_image(image_path)
                    print(f"\n  {Colors.BOLD}Extracted Text:{Colors.RESET}")
                    print(f"  {Colors.DIM}{'â”€' * 55}{Colors.RESET}")
                    for line in result.text.split('\n'):
                        print(f"  {line}")

                # Show metadata
                if result and hasattr(result, 'confidence'):
                    print(f"\n  {Colors.DIM}Confidence: {result.confidence:.0%}{Colors.RESET}")

            except Exception as e:
                print(f"  {Colors.RED}OCR Error: {e}{Colors.RESET}")

            continue

        # Mark user activity - pause autonomous learning while chatting
        if engine_available and autonomous_started and engine.autonomous:
            engine.autonomous.user_active()
            engine.mark_conversation_started()

        # ========================================================================
        # MAIN RESPONSE GENERATION - WITH STREAMING
        # ========================================================================

        history.append({"role": "user", "content": user_input})

        if ollama_available:
            if engine_available:
                # ============================================================
                # NEW: Use the NEURO Engine with streaming
                # ============================================================
                try:
                    # Show cognitive phase: PERCEIVE
                    show_cognitive_phase("perceive", "Retrieving context...")

                    # Get cognitive context
                    cognitive_context = loop.run_until_complete(
                        engine.get_cognitive_context(user_input)
                    )

                    if cognitive_context:
                        memories = cognitive_context.get('memories', [])
                        confidence = cognitive_context.get('confidence', 0)
                        if memories:
                            show_cognitive_phase("perceive", f"{len(memories)} memories, {confidence:.0%} confidence")

                    # Show cognitive phase: THINK
                    show_cognitive_phase("think", "Streaming response...")

                    # Stream the response
                    print(f"\n  {Colors.BRIGHT_GREEN}Neuro:{Colors.RESET} ", end="", flush=True)

                    response = loop.run_until_complete(
                        engine.chat(user_input, history=history[:-1], stream_to_terminal=True)
                    )

                    # Show cognitive phase: LEARN (Real learning!)
                    facts_learned = 0
                    if engine_available and engine.trainer:
                        try:
                            # Extract and learn facts from response
                            facts = engine._extract_facts(response)
                            for fact in facts:
                                if engine.evolution and engine.evolution.is_duplicate(fact['content']):
                                    continue
                                engine.trainer.learn(
                                    topic=fact.get('topic', user_input[:30]),
                                    content=fact['content'],
                                    source="conversation"
                                )
                                if engine.evolution:
                                    engine.evolution.mark_learned(fact['content'])
                                facts_learned += 1

                            if facts_learned > 0:
                                engine.trainer.kb.save()
                                total = engine.trainer.kb.stats.get('total_facts', 0)
                                show_cognitive_phase("learn", f"Learned {facts_learned} facts (total: {total})")
                            else:
                                show_cognitive_phase("learn", "No new facts to learn")
                        except Exception as e:
                            show_cognitive_phase("learn", f"Error: {e}")
                    elif pipeline_available:
                        show_cognitive_phase("learn", "Storing interaction...")
                        try:
                            pipeline.learn(
                                topic="conversation",
                                content=f"Q: {user_input[:100]} A: {response[:200]}",
                                source="chat",
                                importance=0.5
                            )
                        except Exception:
                            pass

                    # Show metadata
                    tools_used = ""
                    learning_str = ""
                    auto_str = ""

                    # Get real learning stats
                    if engine_available and engine.trainer:
                        total_facts = engine.trainer.kb.stats.get('total_facts', 0)
                        learning_str = f" | {Colors.GREEN}{total_facts} facts{Colors.RESET}"

                        if engine.evolution:
                            evo_stats = engine.evolution.get_stats()
                            cycle = evo_stats.get('cycle', 0)
                            progress = evo_stats.get('facts_this_cycle', 0)
                            learning_str += f" | cycle {cycle}"

                            # Show progress bar
                            bar_w = 10
                            filled = int(bar_w * (progress / 100))
                            bar = f"{'â–ˆ' * filled}{'â–‘' * (bar_w - filled)}"
                            learning_str += f" [{bar}]"

                    # Show autonomous status
                    if autonomous_started and engine.autonomous:
                        if engine.autonomous.running:
                            focus = engine.autonomous._current_focus
                            if focus:
                                auto_str = f"\n  {Colors.CYAN}ğŸ¯ Learning from: {focus}{Colors.RESET}"
                            elif engine.autonomous.is_busy:
                                auto_str = f"\n  {Colors.YELLOW}ğŸ”„ Processing...{Colors.RESET}"

                    print(f"\n  {Colors.DIM}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{Colors.RESET}")
                    print(f"  {Colors.DIM}model: {current_model} | streaming{tools_used}{Colors.RESET}{learning_str}")
                    if auto_str:
                        print(auto_str)

                    history.append({"role": "assistant", "content": response})

                    # Learn from response
                    if learner:
                        learner.process_message(response, role="assistant")

                except Exception as e:
                    print(f"\n  {Colors.RED}Error: {e}{Colors.RESET}")
                    # Fall back to non-streaming
                    print(f"  {Colors.DIM}Falling back to non-streaming mode...{Colors.RESET}")

            else:
                # ============================================================
                # FALLBACK: Old synchronous mode (non-streaming)
                # ============================================================
                print(f"\n  {Colors.BRIGHT_GREEN}Neuro:{Colors.RESET} ", end="")
                sys.stdout.flush()

                try:
                    import requests as req
                    import json as js
                    import re

                    # Build enhanced system prompt
                    system_prompt = """You are Neuro, an advanced AGI with neuroscience-inspired cognition.

CORE PRINCIPLES:
1. NEVER LIE - Always be truthful
2. NEVER FABRICATE - Don't make up information
3. BE DIRECT - No unnecessary hedging
4. ADMIT UNCERTAINTY - Say when confidence is low
5. USE TOOLS - When you need external information

Available Tools:
- web_search(query): Search the web
- read_file(path): Read a file
- write_file(path, content): Write to file
- run_command(command): Run shell command

To use a tool: <tool>name</tool><args>{"param": "value"}</args>
"""

                    messages = [{"role": "system", "content": system_prompt}]
                    messages.extend(history[-20:])

                    r = req.post(
                        "http://localhost:11434/api/chat",
                        json={"model": current_model, "messages": messages, "stream": False},
                        timeout=120
                    )
                    response = r.json()["message"]["content"]

                    # Word wrap and print
                    col = 9
                    display_response = re.sub(r'<tool>.*?</tool>', '', response)
                    display_response = re.sub(r'<args>.*?</args>', '', display_response, flags=re.DOTALL)
                    display_response = display_response.strip()

                    for char in display_response:
                        if char == '\n':
                            print()
                            print("         ", end="")
                            col = 9
                        else:
                            if col > 65 and char == ' ':
                                print()
                                print("         ", end="")
                                col = 9
                            else:
                                print(char, end="")
                                col += 1
                    print()

                    print(f"\n  {Colors.DIM}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{Colors.RESET}")
                    print(f"  {Colors.DIM}model: {current_model} | fallback mode{Colors.RESET}")

                    history.append({"role": "assistant", "content": response})

                except Exception as e:
                    print(f"\n  {Colors.RED}Error: {e}{Colors.RESET}")
        else:
            print(f"\n  {Colors.BRIGHT_GREEN}Neuro:{Colors.RESET} ", end="")
            print(f"Ollama is not running. Please start it:")
            print(f"         {Colors.DIM}ollama serve && ollama pull ministral-3:8b{Colors.RESET}")
            print(f"\n  {Colors.DIM}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€{Colors.RESET}")

    return 0


def cmd_info(args):
    """Show system information."""
    print_header()

    print(f"  {Colors.BOLD}NEURO v2.0 Architecture{Colors.RESET}")
    print(f"  {Colors.DIM}{'â”€' * 50}{Colors.RESET}")
    print()

    features = [
        ("Streaming", "Real-time token output from Ollama"),
        ("Code Editing", "Surgical file edits with colored diffs"),
        ("Parallel Execution", "Async tool execution"),
        ("Git Automation", "Safe commits with secrets detection"),
        ("Cognitive Pipeline", "26 neuroscience-inspired modules"),
        ("Memory & Learning", "Gets smarter the more you use it"),
    ]

    for name, desc in features:
        print(f"  {Colors.GREEN}â—{Colors.RESET} {Colors.BOLD}{name}{Colors.RESET}")
        print(f"    {Colors.DIM}{desc}{Colors.RESET}")
    print()

    print(f"  {Colors.BOLD}Quick Start{Colors.RESET}")
    print(f"  {Colors.DIM}{'â”€' * 50}{Colors.RESET}")
    print(f"  {Colors.CYAN}neuro{Colors.RESET}         Start chatting (streaming mode)")
    print(f"  {Colors.CYAN}neuro check{Colors.RESET}   Verify installation")
    print(f"  {Colors.CYAN}neuro demo{Colors.RESET}    Run demonstration")
    print()

    return 0


def cmd_demo(args):
    """Run system demonstration."""
    print_header()
    print(f"  {Colors.BOLD}System Demonstration{Colors.RESET}")
    print(f"  {Colors.DIM}{'â”€' * 50}{Colors.RESET}")
    print()

    # Test streaming engine
    spinner = Spinner("Loading streaming engine")
    spinner.start()

    try:
        sys.path.insert(0, str(Path(__file__).parent))
        from engine import NeuroEngine, EngineConfig
        config = EngineConfig(model="ministral-3:8b", show_thinking=False)
        engine = NeuroEngine(config)
        spinner.stop()
        print(f"  {Colors.GREEN}âœ“{Colors.RESET} Streaming Engine loaded")

        stats = engine.get_stats()
        print(f"    â””â”€ Model: {stats['model']}")
        print(f"    â””â”€ Tools: {'available' if stats['tools_available'] else 'not loaded'}")
        print(f"    â””â”€ Pipeline: {'active' if stats['pipeline_available'] else 'not loaded'}")
        print(f"    â””â”€ Git: {'repo' if stats['git_repo'] else 'no repo'}")

    except Exception as e:
        spinner.stop()
        print(f"  {Colors.RED}âœ—{Colors.RESET} Streaming Engine: {e}")

    # Test code editor
    spinner = Spinner("Loading code editor")
    spinner.start()

    try:
        from editor import CodeEditor
        editor = CodeEditor(auto_confirm=True)
        spinner.stop()
        print(f"  {Colors.GREEN}âœ“{Colors.RESET} Code Editor loaded")
    except Exception as e:
        spinner.stop()
        print(f"  {Colors.RED}âœ—{Colors.RESET} Code Editor: {e}")

    # Test git automator
    spinner = Spinner("Loading git automator")
    spinner.start()

    try:
        from git import GitAutomator
        git = GitAutomator()
        is_repo = git.is_repo()
        spinner.stop()
        print(f"  {Colors.GREEN}âœ“{Colors.RESET} Git Automator loaded")
        print(f"    â””â”€ In repo: {is_repo}")
        if is_repo:
            print(f"    â””â”€ Branch: {git.get_current_branch()}")
    except Exception as e:
        spinner.stop()
        print(f"  {Colors.RED}âœ—{Colors.RESET} Git Automator: {e}")

    # Test executor
    spinner = Spinner("Loading parallel executor")
    spinner.start()

    try:
        from executor import ParallelExecutor
        executor = ParallelExecutor(max_parallel=5)
        spinner.stop()
        print(f"  {Colors.GREEN}âœ“{Colors.RESET} Parallel Executor loaded")
    except Exception as e:
        spinner.stop()
        print(f"  {Colors.RED}âœ—{Colors.RESET} Parallel Executor: {e}")

    print()
    print(f"  {Colors.GREEN}{'â”€' * 50}{Colors.RESET}")
    print(f"  {Colors.GREEN}âœ“ NEURO v2.0 Engine components verified{Colors.RESET}")
    print()

    return 0


def cmd_check(args):
    """Verify installation."""
    print_header()
    print(f"  {Colors.BOLD}Installation Check{Colors.RESET}")
    print(f"  {Colors.DIM}{'â”€' * 50}{Colors.RESET}")
    print()

    checks = []

    # Dependencies
    print(f"  {Colors.BOLD}Dependencies{Colors.RESET}")

    for pkg in ["numpy", "scipy", "aiohttp", "rich"]:
        try:
            mod = __import__(pkg)
            ver = getattr(mod, '__version__', 'ok')
            print(f"  {Colors.GREEN}âœ“{Colors.RESET} {pkg} {Colors.DIM}{ver}{Colors.RESET}")
            checks.append(True)
        except ImportError:
            print(f"  {Colors.YELLOW}!{Colors.RESET} {pkg} {Colors.DIM}(optional){Colors.RESET}")
            checks.append(True)  # Optional deps don't fail

    print()
    print(f"  {Colors.BOLD}NEURO v2.0 Engine{Colors.RESET}")

    components = [
        ("stream", "StreamHandler", "Streaming responses"),
        ("editor", "CodeEditor", "Code editing with diffs"),
        ("executor", "ParallelExecutor", "Parallel tool execution"),
        ("git", "GitAutomator", "Git automation"),
        ("engine", "NeuroEngine", "Unified engine"),
    ]

    for module, cls, desc in components:
        try:
            sys.path.insert(0, str(Path(__file__).parent))
            mod = __import__(module)
            getattr(mod, cls)
            print(f"  {Colors.GREEN}âœ“{Colors.RESET} {cls} {Colors.DIM}({desc}){Colors.RESET}")
            checks.append(True)
        except Exception:
            print(f"  {Colors.RED}âœ—{Colors.RESET} {cls}")
            checks.append(False)

    print()
    print(f"  {Colors.BOLD}Ollama{Colors.RESET}")

    try:
        import requests
        r = requests.get("http://localhost:11434/api/tags", timeout=5)
        if r.status_code == 200:
            models = [m["name"] for m in r.json().get("models", [])]
            print(f"  {Colors.GREEN}âœ“{Colors.RESET} Ollama running")
            if models:
                print(f"  {Colors.GREEN}âœ“{Colors.RESET} Models: {Colors.DIM}{', '.join(models)}{Colors.RESET}")
            else:
                print(f"  {Colors.YELLOW}!{Colors.RESET} No models. Run: {Colors.DIM}ollama pull ministral-3:8b{Colors.RESET}")
            checks.append(True)
        else:
            print(f"  {Colors.YELLOW}!{Colors.RESET} Ollama not responding")
            checks.append(False)
    except Exception:
        print(f"  {Colors.YELLOW}!{Colors.RESET} Ollama not running. Run: {Colors.DIM}ollama serve{Colors.RESET}")
        checks.append(False)

    print()
    passed = sum(checks)
    total = len(checks)

    if passed == total:
        print(f"  {Colors.GREEN}{'â”€' * 50}{Colors.RESET}")
        print(f"  {Colors.GREEN}âœ“ {passed}/{total} checks passed - All systems go{Colors.RESET}")
    else:
        print(f"  {Colors.YELLOW}{'â”€' * 50}{Colors.RESET}")
        print(f"  {Colors.YELLOW}! {passed}/{total} checks passed{Colors.RESET}")

    print()
    return 0 if passed == total else 1


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        prog="neuro",
        description="NEURO AGI v2.0 - Local AI That Learns From Your Code",
    )
    parser.add_argument("-v", "--version", action="store_true", help="Show version")

    subparsers = parser.add_subparsers(dest="command")

    subparsers.add_parser("chat", help="Chat with streaming").set_defaults(func=cmd_chat)
    subparsers.add_parser("info", help="System information").set_defaults(func=cmd_info)
    subparsers.add_parser("demo", help="Run demonstration").set_defaults(func=cmd_demo)
    subparsers.add_parser("check", help="Verify installation").set_defaults(func=cmd_check)

    args = parser.parse_args()

    if args.version:
        print(f"neuro-agi {get_version()}")
        return 0

    if args.command is None:
        # Default: go straight to chat
        return cmd_chat(args)

    return args.func(args)


if __name__ == "__main__":
    sys.exit(main() or 0)
